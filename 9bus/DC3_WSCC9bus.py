from datetime import date, datetime
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import math
import time
from torch.autograd import Function
import torch.functional
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchdiffeq import odeint
#from loguru import logger
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.data import DataLoader, TensorDataset
import argparse
from data import DATA_DIR
import os
from matplotlib.colors import LinearSegmentedColormap
# from src.PINN import PINN
from ODENet import SwingNN
from test import RESULT_DIR
from utils import (balance_constr, bound_constraint, get_Pg_bounded, get_Qg_bounded, get_Vg_bounded, plot_loss, plot_results, read_data)
from save_all import save_data, save_lists
import operator
from functools import reduce
### GENERATOR MODEL PARAM


torch.set_default_dtype(torch.float64)


param = {'w_s': 2*np.pi*60,
        # 'M':         [23.64/(np.pi*60), 6.4/(np.pi*60), 3.01/(np.pi*60)],
         'M':         [47.28, 12.8, 6.2],
        # 'D':         [0., 0., 0.],
        # 'D':         [0.15, 0.15, 0.15],
         'D':         [2.364, 1.28, 0.903],   #From thesis
        #'X_d_prime': [17.5*0.0608, 3.65*0.1198, 6.5*0.1813],
         'X_d_prime': [0.0608, 3.65*0.1198, 0.1813],
         'X_q_prime': [0.0969, 0.1969, 0.2500],
         'H':         [23.64, 6.4, 3.01],
        }

Pm_ = [0.756499419745181, 1.80612485177286, 0.634893365525932]


def dc3_default_args():
    defaults = {}
    defaults['saveAllStats'] = True
    defaults['resultsSaveFreq'] = 50
    defaults['epochs'] = 1000
    defaults['batchSize'] = 200
    defaults['lr'] = 1e-3
    defaults['hiddenSize'] = 200
    defaults['softWeight'] = 10             # use 100 if useCompl=False
    defaults['softWeightEqFrac'] = 0.5
    defaults['useCompl'] = True
    defaults['useTrainCorr'] = True
    defaults['useTestCorr'] = True
    defaults['corrMode'] = 'partial'    # use 'full' if useCompl=False
    defaults['corrTrainSteps'] = 5
    defaults['corrTestMaxSteps'] = 5
    defaults['corrEps'] = 1e-4
    defaults['corrLr'] = 1e-4           # use 1e-5 if useCompl=False
    defaults['corrMomentum'] = 0.5

    return defaults


def get_solution(j, y_zero, t):
    def model_ode(t, U):
        return torch.tensor([param['w_s']*(U[1].cpu().detach().numpy()), 1/(2 * param['H'][j])*(Pm_[j] -param['D'][j] * 2 * np.pi * U[1].cpu().detach().numpy() - U[3].cpu().detach().numpy() * 1/param['X_d_prime'][j] * np.sin(U[0].cpu().detach().numpy() - U[2].cpu().detach().numpy())), 0, 0]).to(device)
    sol = odeint(model_ode, y_zero.squeeze(), t.squeeze()).to(device)
    return sol

#write class for the optimal power flow prediction with a fully connected network (OPF)
class OptimalPowerFlow(nn.Module):  

    # Ask the user to provide inputs  which are the power demand, the number of buses and the number of generators
    # predict the output which is the acive and reactive power generated by each generator, voltage magnitude at each bus, and the angle at each bus
    # compute the power flow equations
    # compute the constraint violation
    # return the loss function
        

    def __init__(self, layers: list, n_bus, n_gen, n_load, Ybus, device, activation=nn.ReLU)->None:
        super().__init__()

        self.layers = layers
        self.n_bus = n_bus
        self.n_gen = n_gen
        self.n_load = n_load
        self.activation = activation
        self.Ybus = Ybus
        self.device = device
        #self.net = self._build_net()

        self.Ybusr = torch.tensor(np.real(Ybus), dtype=torch.get_default_dtype()) #.to(torch.float32)
        self.Ybusi = torch.tensor(np.imag(Ybus), dtype=torch.get_default_dtype()) #.to(torch.float32)

        self.slack = np.zeros(1)
        self.pv = np.array((1,2))
        self.spv = np.concatenate([self.slack, self.pv])  #### GEN BUS INDEXES
        self.spv.sort()
        self.pq = np.setdiff1d(range(self.n_bus), self.spv)  #### NON-GEN BUS INDEXES
        self.nonslack_idxes = np.sort(np.concatenate([self.pq, self.pv]))

        self.pv_ = np.array([np.where(x == self.spv)[0][0] for x in self.pv])

        self.nslack = len(self.slack)
        self.npv = len(self.pv)
        self.pg_pv_zidx = np.arange(self.npv)
        self.vm_spv_zidx = np.arange(self.npv, 2*self.npv + self.nslack)
        self.slack_ = np.array([np.where(x == self.spv)[0][0] for x in self.slack])

        self.pg_start_yidx = 0
        self.qg_start_yidx = self.n_gen
        self.vm_start_yidx = 2*self.n_gen
        self.va_start_yidx = 2*self.n_gen + self.n_bus

        self.pflow_start_eqidx = 0
        self.qflow_start_eqidx = self.n_bus
        self.ydim = 2*self.n_bus + 2*self.n_gen
        self.xdim = 2*self.n_bus

        self.neq = 2*self.n_bus
        self.nineq = 4*self.n_gen + 2*self.n_bus
        self.nknowns = self.nslack

        self.partial_vars = np.concatenate([self.pg_start_yidx + self.pv_, self.vm_start_yidx + self.spv, self.va_start_yidx + self.slack])
        self.other_vars = np.setdiff1d(np.arange(self.ydim), self.partial_vars)
        self.partial_unknown_vars = np.concatenate([self.pg_start_yidx + self.pv_, self.vm_start_yidx + self.spv])

        self.pmax = torch.tensor([2.5000, 2.7000, 2.8500], dtype=torch.get_default_dtype())
        self.pmin = torch.tensor([0.2500, 0.2500, 0.3500], dtype=torch.get_default_dtype())
        self.vmax = torch.tensor([1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1], dtype=torch.get_default_dtype())
        self.vmin = torch.tensor([0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94], dtype=torch.get_default_dtype())
        self.qmax = torch.tensor([0.5500, 0.4000, 0.2000], dtype=torch.get_default_dtype())
        self.qmin = torch.tensor([-0.0500, -0.0500, -0.2000], dtype=torch.get_default_dtype())
        self.slack_va = torch.tensor([0], dtype=torch.get_default_dtype())

        layer_sizes = [2*self.n_bus, 100, 100] ## data.xdim = 18

        self.vm_init = torch.tensor([1.04, 1.045, 1.027, 1.0293, 1.0022, 1.018, 1.0377, 1.021, 1.0387], dtype=torch.get_default_dtype())
        self.va_init = np.array((0, 0.1722, 0.0465, -0.0403, -0.0680, -0.0725, 0.0684, 0.0085, 0.0117))
        self.pg_init = np.array((0.7478, 1.7983, 0.6329)) 
        self.qg_init = np.array((0.2091, 0.2157, -0.1943)) 

        layers = reduce(operator.add,
            [[nn.Linear(a,b), nn.BatchNorm1d(b), self.activation(), nn.Dropout(p=0.2)] 
                for a,b in zip(layer_sizes[0:-1], layer_sizes[1:])])
        output_dim = 2*self.n_bus+2*self.n_gen - 1 ## data.ydim = 24, data.nknowns = 1

        if dc3_default_args()['useCompl']:
            layers += [nn.Linear(layer_sizes[-1], output_dim - self.neq)]
        else:
            layers += [nn.Linear(layer_sizes[-1], output_dim)]

        for layer in layers:
            if type(layer) == nn.Linear:
                nn.init.kaiming_normal_(layer.weight)

        self.net = nn.Sequential(*layers)
        print(self.net)

    def forward(self, x):
        out = self.net(x)
        if dc3_default_args()['useCompl']:
            out = nn.Sigmoid()(out)   # used to interpolate between max and min values
            return self.complete_partial(x, out)
        else:
            return self.process_output(x, out)
        #return self.net(x)

    def eq_resid(self, X, Y):
        pg, qg, vm, va = self.get_yvars(Y)

        vr = vm*torch.cos(va)
        vi = vm*torch.sin(va)

        ## power balance equations
        tmp1 = vr@self.Ybusr - vi@self.Ybusi
        tmp2 = -vr@self.Ybusi - vi@self.Ybusr

        # real power
        pg_expand = torch.zeros(pg.shape[0], self.n_bus, device=self.device)
        pg_expand[:, self.spv] = pg
        real_resid = (pg_expand - X[:,:self.n_bus]) - (vr*tmp1 - vi*tmp2)

        # reactive power
        qg_expand = torch.zeros(qg.shape[0], self.n_bus, device=self.device)
        qg_expand[:, self.spv] = qg
        react_resid = (qg_expand - X[:,self.n_bus:]) - (vr*tmp2 + vi*tmp1)

        ## all residuals
        resids = torch.cat([
            real_resid,
            react_resid
        ], dim=1)
        
        return resids

    def ineq_resid(self, X, Y):
        pg, qg, vm, va = self.get_yvars(Y)
        resids = torch.cat([
            pg - self.pmax,
            self.pmin - pg,
            qg - self.qmax,
            self.qmin - qg,
            vm - self.vmax,
            self.vmin - vm
        ], dim=1)
        return resids

    def ineq_dist(self, X, Y):
        resids = self.ineq_resid(X, Y)
        return torch.clamp(resids, 0)

    def eq_grad(self, X, Y):
        eq_jac = self.eq_jac(Y)
        eq_resid = self.eq_resid(X,Y)
        return 2*eq_jac.transpose(1,2).bmm(eq_resid.unsqueeze(-1)).squeeze(-1)

    def ineq_grad(self, X, Y):
        ineq_jac = self.ineq_jac(Y)
        ineq_dist = self.ineq_dist(X, Y)
        return 2*ineq_jac.transpose(1,2).bmm(ineq_dist.unsqueeze(-1)).squeeze(-1)

    def ineq_partial_grad(self, X, Y):
        eq_jac = self.eq_jac(Y)
        dynz_dz = -torch.inverse(eq_jac[:, :, self.other_vars]).bmm(eq_jac[:, :, self.partial_vars])

        direct_grad = self.ineq_grad(X, Y)
        indirect_partial_grad = dynz_dz.transpose(1,2).bmm(
            direct_grad[:, self.other_vars].unsqueeze(-1)).squeeze(-1)

        full_partial_grad = indirect_partial_grad + direct_grad[:, self.partial_vars]
        full_grad = torch.zeros(X.shape[0], self.ydim, device=self.device)
        full_grad[:, self.partial_vars] = full_partial_grad
        full_grad[:, self.other_vars] = dynz_dz.bmm(full_partial_grad.unsqueeze(-1)).squeeze(-1)
        return full_grad
    
    
    def process_output(self, X, out):
        out2 = nn.Sigmoid()(out[:, :-self.n_bus+self.nslack])
        pg = out2[:, :self.qg_start_yidx] * self.pmax + (1-out2[:, :self.qg_start_yidx]) * self.pmin
        qg = out2[:, self.qg_start_yidx:self.vm_start_yidx] * self.qmax + \
            (1-out2[:, self.qg_start_yidx:self.vm_start_yidx]) * self.qmin
        vm = out2[:, self.vm_start_yidx:] * self.vmax + (1- out2[:, self.vm_start_yidx:]) * self.vmin

        va = torch.zeros(X.shape[0], self.n_bus, device=self.device)
        va[:, self.nonslack_idxes] = out[:, self.va_start_yidx:]
        va[:, self.slack] = torch.tensor(self.slack_va, device=self.device).unsqueeze(0).expand(X.shape[0], self.nslack)

        return torch.cat([pg, qg, vm, va], dim=1)
    

        # Solves for the full set of variables
    def complete_partial(self, X, Z):
        Y_partial = torch.zeros(Z.shape, device=self.device)

        # Re-scale real powers
        Y_partial[:, self.pg_pv_zidx] = Z[:, self.pg_pv_zidx] * self.pmax[1:] + \
             (1-Z[:, self.pg_pv_zidx]) * self.pmin[1:]
        
        # Re-scale real parts of voltages
        Y_partial[:, self.vm_spv_zidx] = Z[:, self.vm_spv_zidx] * self.vmax[self.spv] + \
            (1-Z[:, self.vm_spv_zidx]) * self.vmin[self.spv]

        return self.PFFunction(self)(X, Y_partial)

    
    def grad_steps(self, X, Y, args):
        take_grad_steps = dc3_default_args()['useTrainCorr']
        if take_grad_steps:
            lr = dc3_default_args()['corrLr']
            num_steps = dc3_default_args()['corrTrainSteps']
            momentum = dc3_default_args()['corrMomentum']
            partial_var = dc3_default_args()['useCompl']
            partial_corr = True if dc3_default_args()['corrMode'] == 'partial' else False
            if partial_corr and not partial_var:
                assert False, "Partial correction not available without completion."
            Y_new = Y
            old_Y_step = 0
            for i in range(num_steps):
                if partial_corr:
                    Y_step = self.ineq_partial_grad(X, Y_new)
                else:
                    ineq_step = self.ineq_grad(X, Y_new)
                    eq_step = self.eq_grad(X, Y_new)
                    Y_step = (1 - dc3_default_args()['softWeightEqFrac']) * ineq_step + dc3_default_args()['softWeightEqFrac'] * eq_step
                new_Y_step = lr * Y_step + momentum * old_Y_step
                Y_new = Y_new - new_Y_step
                old_Y_step = new_Y_step
            return Y_new
        else:
            return Y
    

    def grad_steps_all(self, X, Y, args):
        take_grad_steps = dc3_default_args()['useTrainCorr']
        if take_grad_steps:
            lr = dc3_default_args()['corrLr']
            eps_converge = dc3_default_args()['corrEps']
            max_steps = dc3_default_args()['corrTrainSteps']
            momentum = dc3_default_args()['corrMomentum']
            partial_var = dc3_default_args()['useCompl']
            partial_corr = True if dc3_default_args()['corrMode'] == 'partial' else False
            if partial_corr and not partial_var:
                assert False, "Partial correction not available without completion."
            Y_new = Y
            i = 0
            old_Y_step = 0
            old_ineq_step = 0
            old_eq_step = 0
            with torch.no_grad():
                while (i == 0 or torch.max(torch.abs(self.eq_resid(X, Y_new))) > eps_converge or
                            torch.max(self.ineq_dist(X, Y_new)) > eps_converge) and i < max_steps:
                    if partial_corr:
                        Y_step = self.ineq_partial_grad(X, Y_new)
                    else:
                        ineq_step = self.ineq_grad(X, Y_new)
                        eq_step = self.eq_grad(X, Y_new)
                        Y_step = (1 - dc3_default_args()['softWeightEqFrac']) * ineq_step + dc3_default_args()['softWeightEqFrac'] * eq_step
                    new_Y_step = lr * Y_step + momentum * old_Y_step
                    Y_new = Y_new - new_Y_step
                    old_Y_step = new_Y_step
                    i += 1
            return Y_new, i
        else:
            return Y, 0


    def get_yvars(self, y_pred):
        Pg_pred = y_pred[:, :self.n_gen]
        Qg_pred = y_pred[:, self.n_gen:2 * self.n_gen]
        V_pred = y_pred[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus]
        theta_pred = y_pred[:, 2 * self.n_gen + self.n_bus: ]
        return Pg_pred, Qg_pred, V_pred, theta_pred


    def eq_jac(self, Y):
        _, _, vm, va = self.get_yvars(Y)

        # helper functions
        mdiag = lambda v1, v2: torch.diag_embed(v1).bmm(torch.diag_embed(v2))
        Ydiagv = lambda Y, v: Y.unsqueeze(0).expand(v.shape[0], *Y.shape).bmm(torch.diag_embed(v))
        dtm = lambda v, M: torch.diag_embed(v).bmm(M)

        # helper quantities
        cosva = torch.cos(va)
        sinva = torch.sin(va)
        vr = vm * torch.cos(va)
        vi = vm * torch.sin(va)

        Yr = self.Ybusr
        Yi = self.Ybusi
        YrvrYivi = vr@Yr - vi@Yi
        YivrYrvi = vr@Yi + vi@Yr
        spv = [0,1,2]

        # real power equations
        dreal_dpg = torch.zeros(self.n_bus, self.n_gen, device=self.device) 
        dreal_dpg[spv, :] = torch.eye(self.n_gen, device=self.device)
        dreal_dvm = -mdiag(cosva, YrvrYivi) - dtm(vr, Ydiagv(Yr, cosva)-Ydiagv(Yi, sinva)) \
            -mdiag(sinva, YivrYrvi) - dtm(vi, Ydiagv(Yi, cosva)+Ydiagv(Yr, sinva))
        dreal_dva = -mdiag(-vi, YrvrYivi) - dtm(vr, Ydiagv(Yr, -vi)-Ydiagv(Yi, vr)) \
            -mdiag(vr, YivrYrvi) - dtm(vi, Ydiagv(Yi, -vi)+Ydiagv(Yr, vr))
        
        # reactive power equations
        dreact_dqg = torch.zeros(self.n_bus, self.n_gen, device=self.device)
        dreact_dqg[spv, :] = torch.eye(self.n_gen, device=self.device)
        dreact_dvm = mdiag(cosva, YivrYrvi) + dtm(vr, Ydiagv(Yi, cosva)+Ydiagv(Yr, sinva)) \
            -mdiag(sinva, YrvrYivi) - dtm(vi, Ydiagv(Yr, cosva)-Ydiagv(Yi, sinva))
        dreact_dva = mdiag(-vi, YivrYrvi) + dtm(vr, Ydiagv(Yi, -vi)+Ydiagv(Yr, vr)) \
            -mdiag(vr, YrvrYivi) - dtm(vi, Ydiagv(Yr, -vi)-Ydiagv(Yi, vr))

        jac = torch.cat([
            torch.cat([dreal_dpg.unsqueeze(0).expand(vr.shape[0], *dreal_dpg.shape), 
                torch.zeros(vr.shape[0], self.n_bus, self.n_gen, device=self.device), 
                dreal_dvm, dreal_dva], dim=2),
            torch.cat([torch.zeros(vr.shape[0], self.n_bus, self.n_gen, device=self.device), 
                dreact_dqg.unsqueeze(0).expand(vr.shape[0], *dreact_dqg.shape),
                dreact_dvm, dreact_dva], dim=2)],
            dim=1)
        return jac


    def ineq_jac(self, Y):
        jac = torch.cat([
            torch.cat([torch.eye(self.n_gen, device=self.device), 
                torch.zeros(self.n_gen, self.n_gen, device=self.device), 
                torch.zeros(self.n_gen, self.n_bus, device=self.device), 
                torch.zeros(self.n_gen, self.n_bus, device=self.device)], dim=1),
            torch.cat([-torch.eye(self.n_gen, device=self.device), 
                torch.zeros(self.n_gen, self.n_gen, device=self.device), 
                torch.zeros(self.n_gen, self.n_bus, device=self.device), 
                torch.zeros(self.n_gen, self.n_bus, device=self.device)], dim=1),
            torch.cat([torch.zeros(self.n_gen, self.n_gen, device=self.device),
                torch.eye(self.n_gen, device=self.device), 
                torch.zeros(self.n_gen, self.n_bus, device=self.device), 
                torch.zeros(self.n_gen, self.n_bus, device=self.device)], dim=1),
            torch.cat([torch.zeros(self.n_gen, self.n_gen, device=self.device), 
                -torch.eye(self.n_gen, device=self.device),
                torch.zeros(self.n_gen, self.n_bus, device=self.device), 
                torch.zeros(self.n_gen, self.n_bus, device=self.device)], dim=1),
            torch.cat([torch.zeros(self.n_bus, self.n_gen, device=self.device),
                torch.zeros(self.n_bus, self.n_gen, device=self.device), 
                torch.eye(self.n_bus, device=self.device), 
                torch.zeros(self.n_bus, self.n_bus, device=self.device)], dim=1),
            torch.cat([torch.zeros(self.n_bus, self.n_gen, device=self.device), 
                torch.zeros(self.n_bus, self.n_gen, device=self.device),
                -torch.eye(self.n_bus, device=self.device), 
                torch.zeros(self.n_bus, self.n_bus, device=self.device)], dim=1)
            ], dim=0)
        return jac.unsqueeze(0).expand(Y.shape[0], *jac.shape)
    

    def PFFunction(self, tol=1e-5, bsz=200, max_iters=50):
        class PFFunctionFn(Function):
            @staticmethod
            def forward(ctx, X, Z):

                ## Step 1: Newton's method
                Y = torch.zeros(X.shape[0], self.ydim, device=device)
                
                # known/estimated values (pg at pv buses, vm at all gens, va at slack bus)
                Y[:, self.pg_start_yidx + self.pv_] = Z[:, self.pg_pv_zidx]    # pg at non-slack gens
                Y[:, self.vm_start_yidx + self.spv] = Z[:, self.vm_spv_zidx]   # vm at gens
                Y[:, self.va_start_yidx + self.slack] = torch.tensor(self.slack_va, device=device)  # va at slack bus

                # init guesses for remaining values
                Y[:, self.vm_start_yidx + self.pq] = torch.tensor(self.vm_init[self.pq], device=device)  # vm at load buses
                Y[:, self.va_start_yidx + self.pv] = torch.tensor(self.va_init[self.pv], device=device)  # va at non-slack gens 
                Y[:, self.va_start_yidx + self.pq] = torch.tensor(self.va_init[self.pq], device=device)  # va at load buses
                Y[:, self.qg_start_yidx:self.qg_start_yidx+self.n_gen] = 0    # qg at gens (not used in Newton upd)
                Y[:, self.pg_start_yidx+self.slack_] = 0                   # pg at slack (not used in Newton upd)

                keep_constr = np.concatenate([
                    self.pflow_start_eqidx + self.pv,     # real power flow at non-slack gens
                    self.pflow_start_eqidx + self.pq,     # real power flow at load buses
                    self.qflow_start_eqidx + self.pq])    # reactive power flow at load buses
                newton_guess_inds = np.concatenate([             
                    self.vm_start_yidx + self.pq,         # vm at load buses
                    self.va_start_yidx + self.pv,         # va at non-slack gens
                    self.va_start_yidx + self.pq])        # va at load buses

                converged = torch.zeros(X.shape[0])
                jacs = []
                newton_jacs_inv = []
                for b in range(0, X.shape[0], bsz):
                    # print('batch: {}'.format(b))
                    X_b = X[b:b+bsz]
                    Y_b = Y[b:b+bsz]

                    for i in range(max_iters):
                        # print(i)
                        gy = self.eq_resid(X_b, Y_b)[:, keep_constr]
                        jac_full = self.eq_jac(Y_b)
                        jac = jac_full[:, keep_constr, :]
                        newton_jac_inv = torch.inverse(jac[:, :, newton_guess_inds])
                        delta = newton_jac_inv.bmm(gy.unsqueeze(-1)).squeeze(-1)
                        Y_b[:, newton_guess_inds] -= delta
                        if torch.norm(delta, dim=1).abs().max() < 1e-5:
                            break

                    converged[b:b+bsz] = (delta.abs() < 1e-5).all(dim=1)
                    jacs.append(jac_full)
                    newton_jacs_inv.append(newton_jac_inv)

                ## Step 2: Solve for remaining variables

                # solve for qg values at all gens (note: requires qg in Y to equal 0 at start of computation)
                Y[:, self.qg_start_yidx:self.qg_start_yidx + self.n_gen] = \
                    -self.eq_resid(X, Y)[:, self.qflow_start_eqidx + self.spv]
                # solve for pg at slack bus (note: requires slack pg in Y to equal 0 at start of computation)
                Y[:, self.pg_start_yidx + self.slack_] = \
                    -self.eq_resid(X, Y)[:, self.pflow_start_eqidx + self.slack]

                ctx.data = self
                ctx.save_for_backward(torch.cat(jacs), torch.cat(newton_jacs_inv),
                    torch.tensor(newton_guess_inds, device=device), 
                    torch.tensor(keep_constr, device=device))
                return Y


            @staticmethod
            def backward(ctx, dl_dy):

                data = ctx.data
                jac, newton_jac_inv, newton_guess_inds, keep_constr = ctx.saved_tensors

                ## Step 2 (calc pg at slack and qg at gens)

                # gradient of all voltages through step 3 outputs
                last_eqs = np.concatenate([self.pflow_start_eqidx + self.slack, self.qflow_start_eqidx + self.spv])
                last_vars = np.concatenate([
                    self.pg_start_yidx + self.slack_, np.arange(self.qg_start_yidx, self.qg_start_yidx + self.n_gen)])
                jac3 = jac[:, last_eqs, :]
                dl_dvmva_3 = -jac3[:, :, self.vm_start_yidx:].transpose(1,2).bmm(
                    dl_dy[:, last_vars].unsqueeze(-1)).squeeze(-1)

                # gradient of pd at slack and qd at gens through step 3 outputs
                dl_dpdqd_3 = dl_dy[:, last_vars]

                # insert into correct places in x and y loss vectors
                dl_dy_3 = torch.zeros(dl_dy.shape, device=device)
                dl_dy_3[:, self.vm_start_yidx:] = dl_dvmva_3

                dl_dx_3 = torch.zeros(dl_dy.shape[0], self.xdim, device=device)
                dl_dx_3[:, np.concatenate([self.slack, self.n_bus + self.spv])] = dl_dpdqd_3
                ## Step 1
                dl_dy_total = dl_dy_3 + dl_dy  # Backward pass vector including result of last step

                # Use precomputed inverse jacobian
                jac2 = jac[:, keep_constr, :]
                d_int = newton_jac_inv.transpose(1,2).bmm(
                                dl_dy_total[:,newton_guess_inds].unsqueeze(-1)).squeeze(-1)

                dl_dz_2 = torch.zeros(dl_dy.shape[0], self.npv + self.n_gen, device=device)
                dl_dz_2[:, self.pg_pv_zidx] = -d_int[:, :self.npv]  # dl_dpg at pv buses
                dl_dz_2[:, self.vm_spv_zidx] = -jac2[:, :, self.vm_start_yidx + self.spv].transpose(1,2).bmm(
                    d_int.unsqueeze(-1)).squeeze(-1)

                dl_dx_2 = torch.zeros(dl_dy.shape[0], self.xdim, device=device)
                dl_dx_2[:, self.pv] = d_int[:, :self.npv]                       # dl_dpd at pv buses
                dl_dx_2[:, self.pq] = d_int[:, self.npv:self.npv+len(self.pq)]  # dl_dpd at pq buses
                dl_dx_2[:, self.n_bus + self.pq] = d_int[:, -len(self.pq):]      # dl_dqd at pq buses

                # Final quantities
                dl_dx_total = dl_dx_3 + dl_dx_2
                dl_dz_total = dl_dz_2 + dl_dy_total[:, np.concatenate([
                    self.pg_start_yidx + self.pv_, self.vm_start_yidx + self.spv])]

                return dl_dx_total, dl_dz_total

        return PFFunctionFn.apply



    def compute_loss(self, x, y, DATA_bounds, DATA_balance, Ybus, c, 
                     NODE_models, t, epoch, batch_idx, args, collected, saved, device ):
        #print("Batch idx: ", batch_idx)
        x_expand = torch.zeros(x.shape[0], 2*self.n_bus)
        x_expand[:, [4,5,7]] = x[:,:n_loads]
        x_expand[:, [self.n_bus+4, self.n_bus+5, self.n_bus+7]] = x[:,n_loads:]
        start_time = time.time()
        y_pred = self.forward(x_expand)

        Ynew_train, steps = self.grad_steps_all(x_expand, y_pred, args)
        Ynew_train = self.grad_steps(x_expand, y_pred, args)
        # Ynew_train = self.grad_steps(x, y_pred, args, Ybus)
        # Ycorr, steps = grad_steps_all(data, X, Y, args)
        end_time = time.time()
        #print(end_time-start_time)
        Pg_pred = Ynew_train[:, :self.n_gen]
        Qg_pred = Ynew_train[:, self.n_gen:2 * self.n_gen]
        V_pred = Ynew_train[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus]
        theta_pred = Ynew_train[:, 2 * self.n_gen + self.n_bus: ]
        # Pg_pred = get_Pg_bounded(Pg_pred, DATA_bounds, device).to(device)#.float()
        # Qg_pred = get_Qg_bounded(Qg_pred, DATA_bounds, device).to(device)#.float()
        # V_pred = get_Vg_bounded(V_pred, DATA_bounds, device).to(device)#.float()

        delta_pred_dyn = torch.zeros((Ynew_train.shape[0], n_gen), device=device)

        total_true_unstable, total_false_unstable, total_detected_true_unstable = 0, 0, 0
        
        #collect = False
        #if np.random.randint(1,3)==1:
        #    collect = True 

        if epoch>args['activate_instability_computation_epoch']: ### ACTIVATE INSTABILITY LOSS COMPUTATION AFTER 1 (HYPERPARAM) EPOCH
            for k, z in enumerate(DATA_balance['idx_Gbus'][:3]):
                if k==1:
                    ### RETRIEVING PRE-GENERATED INITIAL CONDITION  (\delta(0) AND \omega(0)) and ATTACHING THEM TO THE PREDICTED V AND \THETA                
                    if batch_idx>=0:
                        x_input = torch.cat((init_cond[batch_idx*args['batchsize']:(batch_idx+1)*args['batchsize'],k,:], theta_pred[:, z].reshape((-1,1)), V_pred[:, z].reshape((-1,1))), dim=1).to(device)
                    elif batch_idx==-1:
                        #x_input = torch.cat((init_cond[80:160,k,:], theta_pred[:, z].reshape((-1,1)), V_pred[:, z].reshape((-1,1))), dim=1).to(device)
                        x_input = torch.cat((init_cond[800:880,k,:], theta_pred[:, z].reshape((-1,1)), V_pred[:, z].reshape((-1,1))), dim=1).to(device)
                    elif batch_idx==-10:
                        x_input = torch.cat((init_cond[880:,k,:], theta_pred[:, z].reshape((-1,1)), V_pred[:, z].reshape((-1,1))), dim=1).to(device)
                        #x_input = torch.cat((init_cond[160:,k,:], theta_pred[:, z].reshape((-1,1)), V_pred[:, z].reshape((-1,1))), dim=1).to(device)
                    if k==0:
                        y_pred_dyn = odeint(NODE_models[f'gen{z}'], x_input, t.squeeze(), method='rk4') #
                    else:
                        y_pred_dyn = odeint(NODE_models[f'gen{z}'], x_input, t.squeeze()[:int(.3*t.shape[0])], method='rk4')
                    
                    y_pred_dyn = torch.swapaxes(y_pred_dyn, 0, 1) # batch, time, features = 4
                    delta_pred_dyn[:, k] = y_pred_dyn[:, -1, 0]
                    mask = torch.abs(delta_pred_dyn[:, k]) > torch.pi/2
                    total_detected_true_unstable += torch.sum(mask).item()
                    #print(f"K: {k+1}, num. of detected unstable trajectory: {torch.sum(mask)}")
                    #delta_pred_dyn[mask, k] = torch.pi
                    not_mask = ~mask
                    #delta_pred_dyn[not_mask, k] = 0
                    #indices = torch.nonzero(mask, as_tuple=False)   ### USED TO COMPUTE THE TRUE TRAJECTORIES ONLY WHEN THE NEURAL ODE MODEL DETECTS UN UNSTABLE TRAJECTORY
                    if batch_idx==-1:
                        if k==0:
                            time_vector = t
                        else:
                            time_vector = t[:int(.3*t.shape[0])]
                        time_vector = time_vector.to(device)
                        #y_true_dyn = torch.zeros((indices.shape[0],time_vector.shape[0],4))
                        y_true_dyn = torch.zeros((y_pred_dyn.shape[0],time_vector.shape[0],4)).to(device)
            
                        for p in range(y_pred_dyn.shape[0]):
                        #for p,idx in enumerate(indices):
                            y_true_dyn[p,:,:] = get_solution(k, x_input[p,:], time_vector).to(device)
            
                        true_unstable = torch.abs(y_true_dyn[:, -1,0]) > torch.pi/2
                        total_true_unstable += torch.sum(true_unstable).item()
                        print(f"K: {k+1}, num. of true unstable trajectory: {torch.sum(true_unstable)}")
                        #print(f"K: {k+1}, num. of true unstable trajectory with V=Vmax: {torch.sum(unstable_max)}")
                        total_false_unstable += torch.sum(mask & ~true_unstable).item()
                        
                        # if epoch>1 and collect and batch_idx!=-1 and collected<theta_pred_data.shape[0]:
                        #     theta_pred_data[collected:collected+args['batchsize'], k] = theta_pred[:args['batchsize'],k]
                        #     V_pred_data[collected:collected+args['batchsize'], k] = V_pred[:args['batchsize'],k]
                        #     collected += args['batchsize']
            
                        # if collected == theta_pred_data.shape[0]:
                        #     torch.save(theta_pred_data, f'theta_pred_data_{args["id"]}.pt')
                        #     torch.save(V_pred_data, f'V_pred_data_{args["id"]}.pt')
                        
                        #if batch_idx>50:
                        #    # Print the indices
                        #    print("Generator : ",k+1)
                        #    print("Indices where abs(delta_pred_dyn[:, k]) > pi/2:")
                        #    print(indices)
            loss_instability = torch.sum(F.relu(torch.abs(delta_pred_dyn) - torch.pi/2))
        else:
            loss_instability = torch.tensor([0]).to(device)
            total_true_unstable, total_false_unstable, total_detected_true_unstable = -1, -1, -1
            
        #loss_instability = torch.sum(delta_pred_dyn)/delta_pred_dyn.shape[0]
        #tmp = (balance_constr(y[:, :self.n_gen], y[:, self.n_gen:2 * self.n_gen], y[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus], y[: , 2 * self.n_gen + self.n_bus: ], x[:, :self.n_load].float(), x[:, self.n_load:2*self.n_load].float() , DATA_balance, Ybus, device) - balance_constr(Pg_pred, Qg_pred, V_pred, theta_pred, x[:, :self.n_load].float(), x[:, self.n_load:2*self.n_load].float() , DATA_balance, Ybus, device))**2
        
        loss_balance = torch.max(torch.abs(self.eq_resid(x_expand, Ynew_train)))

        loss_bc = bound_constraint(Pg_pred, Qg_pred, V_pred, theta_pred, DATA_bounds, device = device)
        loss_pred_p = F.mse_loss(Pg_pred, y[:, :self.n_gen])
        loss_pred_q = F.mse_loss(Qg_pred, y[:, self.n_gen:2 * self.n_gen])
        loss_pred_V = F.mse_loss(V_pred, y[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus])
        loss_pred_theta = F.mse_loss(theta_pred, y[: , 2 * self.n_gen + self.n_bus: ])
        loss = loss_pred_p + loss_pred_q + loss_pred_V + loss_pred_theta + c[0] * loss_balance + c[1] * loss_bc #+ c[2] * loss_instability

        if batch_idx==40:
            print("Pg loss: ", loss_pred_p)
            print("Qg loss: ", loss_pred_q)
            print("V loss: ", loss_pred_V)
            print("Theta loss: ", loss_pred_theta)
            print("Balance loss: ", loss_balance)
            print("Boundary loss: ", loss_bc)
            print("Instability loss: ", loss_instability)
        if batch_idx!=-10:
            return loss, [loss_pred_p.item(), loss_pred_q.item(), loss_pred_V.item(), loss_pred_theta.item(), loss_balance.item(), loss_bc.item(), loss_instability.item()], total_true_unstable, total_false_unstable, total_detected_true_unstable
        else:
            return loss, [loss_pred_p.item(), loss_pred_q.item(), loss_pred_V.item(), loss_pred_theta.item(), loss_balance.item(), loss_bc.item(), loss_instability.item()], total_true_unstable, total_false_unstable, total_detected_true_unstable, Ynew_train
        

    def compute_loss_and_stats(self, x, y, DATA_bounds, DATA_balance, Ybus, c, 
                     NODE_models, t, epoch, batch_idx, args, collected, saved, device ):
        #print("Batch idx: ", batch_idx)
        # y_pred = self.net(x[:, :2*self.n_load].float())
        # Pg_pred = y_pred[:, :self.n_gen]
        # Qg_pred = y_pred[:, self.n_gen:2 * self.n_gen]
        # V_pred = y_pred[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus]
        # theta_pred = y_pred[:, 2 * self.n_gen + self.n_bus: ]
        # Pg_pred = get_Pg_bounded(Pg_pred, DATA_bounds, device).to(device).float()
        # Qg_pred = get_Qg_bounded(Qg_pred, DATA_bounds, device).to(device).float()
        # V_pred = get_Vg_bounded(V_pred, DATA_bounds, device).to(device).float()
        

        x_expand = torch.zeros(x.shape[0], 2*self.n_bus)
        x_expand[:, [4,5,7]] = x[:,:n_loads]
        x_expand[:, [self.n_bus+4, self.n_bus+5, self.n_bus+7]] = x[:,n_loads:]

        y_pred = self.forward(x_expand)

        Ynew_train, steps = self.grad_steps_all(x_expand, y_pred, args)
        Ynew_train = self.grad_steps(x_expand, y_pred, args)
        # Ynew_train = self.grad_steps(x, y_pred, args, Ybus)
        # Ycorr, steps = grad_steps_all(data, X, Y, args)
        # end_time = time.time()

        Pg_pred = Ynew_train[:, :self.n_gen]
        Qg_pred = Ynew_train[:, self.n_gen:2 * self.n_gen]
        V_pred = Ynew_train[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus]
        theta_pred = Ynew_train[:, 2 * self.n_gen + self.n_bus: ]


        delta_pred_dyn = torch.zeros((y_pred.shape[0], n_gen), device=device)

        total_true_unstable, total_false_unstable, total_detected_true_unstable = 0, 0, 0
        
        for k, z in enumerate(DATA_balance['idx_Gbus'][:3]):
            if k==1:
                ### RETRIEVING PRE-GENERATED INITIAL CONDITION  (\delta(0) AND \omega(0)) and ATTACHING THEM TO THE PREDICTED V AND \THETA                
                x_input = torch.cat((init_cond[880+batch_idx:880+batch_idx+1,k,:], theta_pred[:, z].reshape((-1,1)), V_pred[:, z].reshape((-1,1))), dim=1).to(device)
                #x_input = torch.cat((init_cond[160+batch_idx:160+batch_idx+1,k,:], theta_pred[:, z].reshape((-1,1)), V_pred[:, z].reshape((-1,1))), dim=1).to(device)
                if k==0:
                    y_pred_dyn = odeint(NODE_models[f'gen{z}'], x_input, t.squeeze(), method='rk4') #
                else:
                    y_pred_dyn = odeint(NODE_models[f'gen{z}'], x_input, t.squeeze()[:int(.3*t.shape[0])], method='rk4')
                
                y_pred_dyn = torch.swapaxes(y_pred_dyn, 0, 1) # batch, time, features = 4

                delta_pred_dyn[:, k] = y_pred_dyn[:, -1, 0]

        loss_instability = torch.sum(F.relu(torch.abs(delta_pred_dyn) - torch.pi/2))

        loss_balance = torch.max(torch.abs(self.eq_resid(x_expand, Ynew_train)))

        loss_bc = bound_constraint(Pg_pred, Qg_pred, V_pred, theta_pred, DATA_bounds, device = device)
        loss_pred_p = F.mse_loss(Pg_pred, y[:, :self.n_gen])
        loss_pred_q = F.mse_loss(Qg_pred, y[:, self.n_gen:2 * self.n_gen])
        loss_pred_V = F.mse_loss(V_pred, y[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus])
        loss_pred_theta = F.mse_loss(theta_pred, y[: , 2 * self.n_gen + self.n_bus: ])
        loss = loss_pred_p + loss_pred_q + loss_pred_V + loss_pred_theta + c[0] * loss_balance + c[1] * loss_bc + c[2] * loss_instability
        return loss, [loss_pred_p.item(), loss_pred_q.item(), loss_pred_V.item(), loss_pred_theta.item(), loss_balance.item(), loss_bc.item(), loss_instability.item()], y_pred

    def train(self, x_train, y_train, x_valid, y_valid, x_test, y_test, DATA_bounds, DATA_balance, Ybus,
              NODE_models, t, args,
              n_epoch, lr, scheduler, optimizer, batch_size, device = 'cpu'):
        
        ### LAGRANGIAN MULTIPLIERS

        c = torch.tensor([args['lambda_zero_balance'], args['lambda_zero_boundaries'], args['lambda_zero_instability']], dtype=torch.float32, device=device)
        
        ### OPTIMIZER AND SCHEDULER

        optimizer = optimizer(self.parameters(), lr=lr)
        scheduler = scheduler(optimizer, milestones=[int(0.5 * n_epoch), int(0.75 * n_epoch)], gamma=0.1)

        ### DATALOADERS

        train_dataset = TensorDataset(x_train, y_train)
        val_dataset = TensorDataset(x_valid, y_valid)
        test_dataset = TensorDataset(x_test, y_test)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)

        ### EARLY STOPPING PARAMETERS & VARIABLES

        max_patience = 10
        patience = 0
        best = 100000

        ### LISTS FOR SAVING LOSS FUNCTION COMPONENTS LISTS

        train_loss, val_loss , test_loss = [], [], [] ### TOTAL LOSS
        mse_loss_list_train, mse_loss_list_valid, mse_loss_list_test = [], [], [] ### MSE LOSS
        balance_loss_list_train, balance_loss_list_valid, balance_loss_list_test = [], [], [] ### BALANCE CONSTRAINT LOSS
        bc_loss_list_train, bc_loss_list_valid, bc_loss_list_test = [], [], [] ### BOUNDARIES CONSTRAINT LOSS
        unstable_loss_list_train, unstable_loss_list_valid, unstable_loss_list_test = [], [], [] ### INSTABILITY PENALTY LOSS
        c_0_list, c_1_list, c_2_list  = [], [], []  ### LAGRANGIAN MULTIPLIER 
        true_unstable_list_train, false_unstable_list_train, detected_unstable_list_train = [], [], [] ## UNSTABLE STATS
        true_unstable_list_valid, false_unstable_list_valid, detected_unstable_list_valid = [], [], []
        true_unstable_list_test, false_unstable_list_test, detected_unstable_list_test = [], [], []

        collected = 0
        saved = False

        for epoch in range(n_epoch):
            batch_loss_train = []
            batch_loss_val = []
            batch_mse_loss_list, batch_balance_loss_list, batch_bc_loss_list, batch_unstable_loss_list = [], [], [], []
            batch_true_unstable_list, batch_false_unstable_list, batch_detected_unstable = [], [], []
            print("Epoch: ", epoch)

            ### TRAINING ROUTINE

            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                loss, loss_item, tu, fu, du = self.compute_loss(data, target, DATA_bounds, DATA_balance, Ybus, c, NODE_models, t, epoch, batch_idx, args, collected, saved, device)
                loss.backward()

                batch_mse_loss_list.append(loss_item[0]+loss_item[1]+loss_item[2]+loss_item[3])
                batch_balance_loss_list.append(loss_item[4])
                batch_bc_loss_list.append(loss_item[5])
                batch_unstable_loss_list.append(loss_item[6])
                batch_loss_train.append(loss.item())

                batch_true_unstable_list.append(tu)
                batch_false_unstable_list.append(fu)
                batch_detected_unstable.append(du)

                optimizer.step()
            
            ### SAVING AVERAGE TRAINING LOSS (PER EPOCH) COMPONENT 
                
            mse_loss_list_train.append(np.mean(batch_mse_loss_list))
            balance_loss_list_train.append(np.mean(batch_balance_loss_list))
            bc_loss_list_train.append(np.mean(batch_bc_loss_list)) 
            unstable_loss_list_train.append(np.mean(batch_unstable_loss_list))
            train_loss.append(np.mean(batch_loss_train))

            ### SAVING AVERAGE TRUE, FALSE AND DETECTED UNSTABLE (PER EPOCH) 

            true_unstable_list_train.append(np.mean(batch_true_unstable_list))
            false_unstable_list_train.append(np.mean(batch_false_unstable_list))
            detected_unstable_list_train.append(np.mean(batch_detected_unstable))

            if epoch % 1 == 0:
                with torch.no_grad():

                    ### VALIDATION ROUTINE

                    for x_val, y_val in val_loader:
                        loss_val, loss_item, tu, fu, du  = self.compute_loss(x_val, y_val, DATA_bounds, DATA_balance, Ybus, c, NODE_models, t, epoch, -1, args, collected, saved, device)
                        mse_loss_list_valid.append(loss_item[0]+loss_item[1]+loss_item[2]+loss_item[3])
                        balance_loss_list_valid.append(loss_item[4])
                        bc_loss_list_valid.append(loss_item[5])
                        unstable_loss_list_valid.append(loss_item[6])
                        batch_loss_val.append(loss_val.item())

                        true_unstable_list_valid.append(tu)
                        false_unstable_list_valid.append(fu)
                        detected_unstable_list_valid.append(du)

                val_loss.append(np.mean(batch_loss_val))
                
                ### LAGRANGIAN MULTIPLIERS UPDATE

                c[0] = c[0] + args['ldstepsize'] * loss_item[-3]  ### BALANCE COSTRAINT MULTIPLIER
                c[1] = c[1] + args['ldstepsize'] * loss_item[-2]  ### BOUNDARY CONSTRAINT MULTIPLIER
                c[2] = c[2] + args['ldstepsize'] * loss_item[-1]  ### INSTABILITY PENALTY MULTIPLIER
                
                c_0_list.append(c[0].item())
                c_1_list.append(c[1].item())
                c_2_list.append(c[2].item())
                
            if loss_val<best:
                best=loss_val
                ### SAVING BEST MODELS
                #torch.save(self.state_dict(), f'best_model_DYN/model_OPF_{args["id"]}.pth')
                patience = 0
            else:
                patience += 1
                scheduler.step()
                if patience == max_patience:
                    break
        
        ### TEST ROUTINE
        
        #self.load_state_dict(torch.load(f'best_model_DYN/model_OPF_{args["id"]}.pth'))
        batch_loss_test = []
        gen_cost = torch.tensor([285/250, 285/270, 1]).to(device)

        with torch.no_grad():
            for x_test, y_test in test_loader:
                loss_test, loss_item, tu, fu, du, y_pred  = self.compute_loss(x_test, y_test, DATA_bounds, DATA_balance, Ybus, c, NODE_models, t, epoch, -10, args, collected, saved, device)
                mse_loss_list_test.append(loss_item[0]+loss_item[1]+loss_item[2]+loss_item[3])
                balance_loss_list_test.append(loss_item[4])
                bc_loss_list_test.append(loss_item[5])
                unstable_loss_list_test.append(loss_item[6])
                batch_loss_test.append(loss_test.item())
                true_unstable_list_test.append(tu)
                false_unstable_list_test.append(fu)
                detected_unstable_list_test.append(du)
            test_loss.append(np.mean(batch_loss_test))

        optimality_gap = np.zeros((test_loader.batch_size))
        avg_optimality_gap = optimality_gap.mean(axis=0)
        std_dev_optimality_gap = optimality_gap.std(axis=0)

        for i in range(test_loader.batch_size):
            relative_opt_gap = torch.abs(torch.sum(gen_cost*(y_pred[i,:n_gen]-y_test[i,:n_gen])))/torch.sum(gen_cost*(y_test[i,:n_gen]))
            optimality_gap[i] = relative_opt_gap.item()*100

        ### COMPUTE STATISTICS PER LOAD
        
        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
        batch_loss_test = []
        
        n_bins = 100
        pg_mse, v_mse, theta_mse = np.zeros((len(test_loader),n_bins)), np.zeros((len(test_loader),n_bins)), np.zeros((len(test_loader),n_bins))
        balance_error, bc_error, unstable_error = np.zeros((len(test_loader),n_bins)), np.zeros((len(test_loader),n_bins)), np.zeros((len(test_loader),n_bins))

        # optimality_gap = np.zeros((len(test_loader),n_bins))
        # sd_interval_length = 1.9  ### MAX_SD_MAX - MIN_SD_MIN, OUTER MAX ACROSS ALL LOADS, INNER MAX PER LOAD
        # sd_max = 2.5 
        # gen_cost = torch.tensor([285/250, 285/270, 1]).to(device)

        # with torch.no_grad():
        #     for n_sample,(x_test, y_test) in enumerate(test_loader):
        #         loss_test, loss_item, y_pred  = self.compute_loss_and_stats(x_test, y_test, DATA_bounds, DATA_balance, Ybus, c, NODE_models, t, epoch, n_sample, args, collected, saved, device)
        #         x_test = x_test.squeeze(0)
        #         avg_load = sum([(x_test[i]**2+x_test[n_loads+i]**2)/3 for i in range(n_loads)]).item()
        #         tmp = int(n_bins/sd_interval_length*(sd_max-avg_load))
        #         #print("Sd**2: ", x_test[idx]**2+x_test[idx+n_loads]**2)
        #         if tmp<0 or tmp>n_bins:
        #             print("OOOPS")
        #         pg_mse[n_sample, tmp] = loss_item[0]
        #         v_mse[n_sample, tmp] = loss_item[2]
        #         theta_mse[n_sample, tmp] = loss_item[3]
        #         balance_error[n_sample, tmp] = loss_item[4]
        #         bc_error[n_sample, tmp] = loss_item[5]
        #         unstable_error[n_sample, tmp] = loss_item[6]
        #         relative_opt_gap = torch.abs(torch.sum(gen_cost*(y_pred.squeeze(0)[:n_gen]-y_test.squeeze(0)[:n_gen])))/torch.sum(gen_cost*(y_test.squeeze(0)[:n_gen]))
        #         optimality_gap[n_sample, tmp] = relative_opt_gap.item()*100
        
        avg_pg_mse, avg_v_mse, avg_theta_mse, avg_balance_error = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)
        avg_bc_error, avg_unstable_error, avg_optimality_gap  = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)
        
        std_dev_pg_mse, std_dev_v_mse, std_dev_theta_mse, std_dev_balance_error = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)
        std_dev_bc_error, std_dev_unstable_error, std_dev_optimality_gap  = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)
        
        ### shape: (n_bins)

        avg_pg_mse = pg_mse.mean(axis=0)
        std_dev_pg_mse = pg_mse.std(axis=0)

        avg_v_mse = v_mse.mean(axis=0)
        std_dev_v_mse = v_mse.std(axis=0)

        avg_theta_mse = theta_mse.mean(axis=0)
        std_dev_theta_mse = theta_mse.std(axis=0)

        avg_balance_error = balance_error.mean(axis=0)
        std_dev_balance_error = balance_error.std(axis=0)

        avg_bc_error = bc_error.mean(axis=0)
        std_dev_bc_error = bc_error.std(axis=0)

        avg_unstable_error = unstable_error.mean(axis=0)
        std_dev_unstable_error = unstable_error.std(axis=0)

        avg_optimality_gap = optimality_gap.mean(axis=0)
        std_dev_optimality_gap = optimality_gap.std(axis=0)
        
        ###############
        #
        ############### SAVING RESULTS  ###############
        #
        ###############

        direc='WSCC9_RESULTS/DC3'
        run_id = args['id'] 

        save_data(direc, run_id, avg_pg_mse, std_dev_pg_mse, avg_v_mse, std_dev_v_mse, avg_theta_mse, std_dev_theta_mse,
                  avg_balance_error, std_dev_balance_error, avg_bc_error, std_dev_bc_error, avg_unstable_error, std_dev_unstable_error, avg_optimality_gap, std_dev_optimality_gap)

        save_lists(direc, run_id, c_0_list, c_1_list, c_2_list, train_loss, mse_loss_list_train, balance_loss_list_train, bc_loss_list_train, unstable_loss_list_train,
                val_loss, mse_loss_list_valid, balance_loss_list_valid, bc_loss_list_valid, unstable_loss_list_valid,
                test_loss, mse_loss_list_test, balance_loss_list_test, bc_loss_list_test, unstable_loss_list_test,
                true_unstable_list_train, false_unstable_list_train, detected_unstable_list_train, 
                true_unstable_list_valid, false_unstable_list_valid, detected_unstable_list_valid,
                true_unstable_list_test, false_unstable_list_test, detected_unstable_list_test, T)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='PI_ROPF')

    ### HYPERPARAMS

    parser.add_argument('--seed', type=int, default=25, help='random seed')
    parser.add_argument('--nHiddenUnit_SS', type=int, default = 50, help='number of hidden units')
    parser.add_argument('--nHiddenUnit_NODE', type=int, default = 200, help='number of hidden units') #50 for the old models
    parser.add_argument('--activation_NODE', type=str, default = "RELU", help='activation_function')
    parser.add_argument('--activation_SS', type=str, default = "RELU", help='activation_function')
    parser.add_argument('--optimizer', type=int, default = 2, help='GD algorithm')
    parser.add_argument('--lr', type=float, default = 1e-3, help='total number of datapoints')
    parser.add_argument('--batchsize', type=int, default = 10, help='training batch size') #10
    parser.add_argument('--epochs', type=int, default = 60, help='training batch size')
    parser.add_argument('--initSplit', type=float, default = .1)
    parser.add_argument('--normalize', type=bool, default = False)
    parser.add_argument('--plot', type=bool, default = False)
    parser.add_argument('--useFFLayer', type=bool, default = True)
    parser.add_argument('--k', type=float, default = .05)
    parser.add_argument('--k_check', type=float, default = .5)
    parser.add_argument('--delta_k', type=float, default = .05)
    parser.add_argument('--delta_k_epoch', type=int, default = 10)
    parser.add_argument('--delta_k_split', type=float, default = 1)
    parser.add_argument('--ldstepsize', type=float, default = .01)
    parser.add_argument('--activate_instability_computation_epoch', type=int, default = 0)
    parser.add_argument('--max_patience', type=int, default = 10)
    parser.add_argument('--nLayer_NODE', type=int, default = 5, help='number of layers')
    parser.add_argument('--nLayer_SS', type=int, default = 5, help='number of layers')
    parser.add_argument('--id', type=int, default = 25, help='number of layers')
    parser.add_argument('--lambda_zero_balance', type=float, default = .1, help='lambda(0)[0]')
    parser.add_argument('--lambda_zero_boundaries', type=float, default = .1, help='lambda(0)[1]')
    parser.add_argument('--lambda_zero_instability', type=float, default = 1, help='lambda(0)[2]')

    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    today = date.today()
    
    args = parser.parse_args()
    args = vars(args) # change to dictionary

    # NETWORK PARAMETERS

    n_bus = 9 #14
    n_gen = 3 #5
    n_loads = 3 #11
    arch_list = [args['nHiddenUnit_SS']] * args['nLayer_SS']
    hidden_layers_nodes = arch_list

    arch_list_NODE = [args['nHiddenUnit_NODE']] * args['nLayer_NODE']
    arch_list_NODE.insert(0,4) 
    arch_list_NODE.append(4)

    T = 10  
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # DATA LOADING

    data, DATA_bounds, DATA_balance, Ybus = read_data(n_bus, n_loads, n_gen)
    DATA_bounds["V_max"][0:n_gen] = 1.1

    Ybus = Ybus.to(device)

    # TRAIN, VALIDATION, TEST SPLIT
    train_data = data.iloc[:int(0.8 * len(data))]
    valid_data = data.iloc[int(0.8 * len(data)):int(0.9 * len(data)), :]
    test_data = data.iloc[int(0.9 * len(data)):, :]

    ### DOWNSAMPLING
    train_data = train_data[:int(.1*train_data.shape[0])]
    valid_data = valid_data[:int(.1*train_data.shape[0])]
    test_data = test_data[:int(.1*train_data.shape[0])]

    # TRAINING SET
    train_x = torch.tensor(train_data.iloc[:, 0:2*n_loads].values)/100 #, dtype=torch.float32)/100
    train_y = torch.tensor(train_data.iloc[:, 2*n_loads: ].values) #, dtype=torch.float32)
    train_x = train_x.to(device)
    train_y = train_y.to(device)

    # VALIDATION SET
    valid_x = torch.tensor(valid_data.iloc[:, 0:2*n_loads].values)/100 #, dtype=torch.float32)/100
    valid_y = torch.tensor(valid_data.iloc[:, 2*n_loads: ].values) #, dtype=torch.float32)
    valid_x = valid_x.to(device)
    valid_y = valid_y.to(device)

    # TEST SET
    test_x = torch.tensor(test_data.iloc[:, 0:2*n_loads].values)/100 #, dtype=torch.float32)/100
    test_y = torch.tensor(test_data.iloc[:, 2*n_loads: ].values) #, dtype=torch.float32)
    test_x = test_x.to(device)
    test_y = test_y.to(device)

    all_loads = torch.cat((train_x, valid_x, test_x)) 


    # LOAD PROFILE (USED FOR STATISTICS)
    '''
    bins = 20
    cmap_name = 'load_gradient'
    colors = [(0, 0.8, 1), (0, 0, 1)]  # Light blue to dark blue
    n_bins = 30  # Use more bins for a smoother gradient
    load_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)

    ### LOAD RANGE: 1.12<Sd(1)<1.58, 0.78< Sd(2)<1.12, 0.87<Sd(3)<1.25

    for i in range(n_loads):
        print(i)
        # Calculate the histogram data
        hist_data = (all_loads[:, i]**2 + all_loads[:, i+n_loads]**2).detach().numpy()
        counts, bin_edges = np.histogram(hist_data, bins=bins, density=True)
        bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])
        # Normalize the bin centers for the colormap
        norm = plt.Normalize(bin_edges.min(), bin_edges.max())
        # Create the histogram using bar chart
        fig, ax_theta = plt.subplots(figsize=(8, 4))
        for edge_left, edge_right, center in zip(bin_edges[:-1], bin_edges[1:], bin_centers):
            color = load_cmap(norm(center))
            ax_theta.bar(center, counts[np.digitize(center, bin_edges) - 1], align='center',
                        width=np.diff(bin_edges)[0], color=color) #, edgecolor='black')
        # Set titles and labels for theta plot
        ax_theta.set_xlabel('Value')
        ax_theta.set_ylabel('Density')
        ax_theta.legend([f'{i+1} Load'])
        plt.tight_layout()
        plt.show()
    '''
    all_data = torch.cat((train_y, valid_y, test_y))

    model = OptimalPowerFlow(hidden_layers_nodes, n_bus, n_gen, n_loads, Ybus, device)
    model = model.to(device)
    
    t = torch.linspace(0, T, 1000).reshape(-1, 1).to(device)
    NODE_models = {}

    # PRETRAINED NODE MODELS LOADING
    pg = all_data[:, :n_gen]
    true_unstable = np.zeros(3)
    qg = all_data[:, n_gen:2 * n_gen]
    V_ = all_data[:, 2 * n_gen:2 * n_gen + n_bus]
    theta_ = all_data[:, 2 * n_gen + n_bus: ]
    pg_nominal = [0.756499419745181, 1.80612485177286, 0.634893365525932]
    qg_nominal = [.2, .2, -0.19]
    ### RANDOM INTIAL CONDITION (\delta(0), \omega(0)) GENERATION (per sample)
    init_cond = torch.zeros((train_x.shape[0]+valid_x.shape[0]+test_x.shape[0],n_gen,2)).to(device)
    t = torch.linspace(0, 3, 100).reshape(-1, 1).to(device)
    y_true_dyn = torch.zeros((80,t.shape[0],4)).to(device)


    for i in range( train_x.shape[0] + valid_x.shape[0] + test_x.shape[0]):
        #init_cond[i,0,:] = torch.cat((0.1 * torch.rand(1), 0.005* torch.rand(1)- 0.0025))
        for j in range(n_gen):
            #e_prime = math.sqrt((pg[i,j].detach().cpu().numpy()*param['X_d_prime'][j])**2+(V_[i,j].detach().cpu().numpy()+qg[i,j].detach().cpu().numpy())**2)/V_[i,j].detach().cpu().numpy()

            e_prime = math.sqrt((pg_nominal[j]*param['X_d_prime'][j])**2+(V_[i,j].detach().cpu().numpy()+qg_nominal[j])**2)/V_[i,j].detach().cpu().numpy()
            delta = np.arcsin(Pm_[j]*param['X_d_prime'][j]/3.65/(V_[i,j].detach().cpu().numpy()*e_prime)) + theta_[i,j].detach().cpu().numpy() #delta_ = (.1)* torch.rand(1) #- 0.25 #+ 0.0407/2
            np.random.seed(i+1)
            if j==1:
                if np.random.randint(1,3)==1:
                    omega_ = (0.015 )* torch.rand(1) #+ 0.01
                else:
                    omega_ = (0.002 )* torch.rand(1) + 0.01
            else:
                omega_ = 0
                
            tmp = torch.tensor([delta,omega_])
            init_cond[i,j,:] = tmp
            #print(tmp)
            if i>=800 and i<880:
                 x_input = torch.tensor([delta, omega_, theta_[i,j], V_[i,j]]).to(device)
                 y_true_dyn[i-800,:,:] = get_solution(j, x_input, t).to(device)
                 tmp = 1 if torch.abs(y_true_dyn[i-800, -1,0]) > torch.pi/2 else 0
                 true_unstable[j] += tmp
                 if tmp ==1:
                     x_input = torch.tensor([delta, omega_, theta_[i,j], 1.08]).to(device)
                     y_true_dyn[i-800,:,:] = get_solution(j, x_input, t).to(device)
                     tmp = 1 if torch.abs(y_true_dyn[i-800, -1,0]) > torch.pi/2 else 0
                     if tmp==1:
                         print("Still unstable with Vm=VMAX")
                         x_input = torch.tensor([delta, omega_/2, theta_[i,j], 1.08]).to(device)
                         y_true_dyn[i-800,:,:] = get_solution(j, x_input, t).to(device)
                         if torch.abs(y_true_dyn[i-800, -1,0]) > torch.pi/2:
                             print("Still unstable with omega'=omega/2")
                         else:
                             init_cond[i,j,:] = torch.tensor([delta,omega_/2])

    print(true_unstable)

    for i in DATA_balance['idx_Gbus'][:3]:
        if i==0:
            arch_list_NODE = [args['nHiddenUnit_NODE']] * 2
        elif i==1:
            arch_list_NODE = [50] * 2
        else:
            arch_list_NODE = [args['nHiddenUnit_NODE']] * args['nLayer_NODE']


        arch_list_NODE.insert(0,4) 
        arch_list_NODE.append(4)

        if i!=1:
            NODE_models[f'gen{i}'] = SwingNN(arch_list_NODE, args['activation_NODE'])
            NODE_models[f'gen{i}'].load_state_dict(torch.load(f'NODE model/9 bus/new_best_model_{i+1}.pt', map_location=torch.device('cpu')))
            NODE_models[f'gen{i}'] = NODE_models[f'gen{i}'].to(device)
        else:
            NODE_models[f'gen{i}'] = SwingNN(arch_list_NODE, 'TANH')
            NODE_models[f'gen{i}'].load_state_dict(torch.load(f'NODE model/9 bus/best_model_{i+1}_9.pt', map_location=torch.device('cpu')))
            NODE_models[f'gen{i}'] = NODE_models[f'gen{i}'].to(device)
    
    ### TRAINING, VALIDATION AND TEST ROUTINE
    
    model.train(train_x, train_y, valid_x, valid_y, test_x, test_y, DATA_bounds, DATA_balance, Ybus, 
                NODE_models, t, args,
                n_epoch = args['epochs'], lr = args['lr'], scheduler = MultiStepLR, optimizer = optim.AdamW, batch_size = args['batchsize'], device = device)
    
    if args['plot']:
        plot_results(model, test_x[:, :2*n_loads].float(), test_y, DATA_bounds, DATA_balance, Ybus, n_bus, n_gen)



