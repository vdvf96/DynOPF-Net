from datetime import date, datetime
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torch.functional
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchdiffeq import odeint
#from loguru import logger
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.data import DataLoader, TensorDataset
import argparse
from data import DATA_DIR
import os
from matplotlib.colors import LinearSegmentedColormap
# from src.PINN import PINN
from ODENet import SwingNN
from test import RESULT_DIR
from utils import (balance_constr, bound_constraint, get_Pg_bounded, get_Qg_bounded, get_Vg_bounded, plot_loss, plot_results, read_data)
from save_all import save_data, save_lists

### GENERATOR MODEL PARAMETERS

param = {
        'w_s': 2*np.pi*60,
        'D':         [2.364, 1.28, 0.903],   #From thesis
        #'X_d_prime': [0.0608, 0.1198, 0.1813],
        'X_d_prime': [17.5*0.0608, 3.65*0.1198, 6.5*0.1813],
        'H':         [23.64, 6.4, 3.01],
        #'M':         [47.28, 12.8, 6.2]
}

Pm_ = [0.756499419745181, 1.80612485177286, 0.634893365525932]

theta_pred_data = torch.zeros((1000,3))
V_pred_data = torch.zeros((1000,3))

#write class for the optimal power flow prediction with a fully connected network (OPF)
class OptimalPowerFlow(nn.Module):  
    # Ask the user to provide inputs  which are the power demand, the number of buses and the number of generators
    # predict the output which is the acive and reactive power generated by each generator, voltage magnitude at each bus, and the angle at each bus
    # compute the power flow equations
    # compute the constraint violation
    # return the loss function
    def __init__(self, layers: list, n_bus, n_gen, n_load, activation=nn.ReLU)->None:
        super().__init__()
        self.layers = layers
        self.n_bus = n_bus
        self.n_gen = n_gen
        self.n_load = n_load
        self.activation = activation
        self.net = self._build_net()
        
        self.pg = nn.Sequential(nn.Linear(self.layers[-1], self.n_gen),  self.activation(), nn.Linear(self.n_gen, self.n_gen), nn.Hardsigmoid())

        self.qg = nn.Sequential(nn.Linear(self.layers[-1], self.n_gen),  self.activation(), nn.Linear(self.n_gen, self.n_gen),nn.Hardsigmoid())

        self.vm = nn.Sequential(nn.Linear(self.layers[-1], self.n_bus),  self.activation(), nn.Linear(self.n_bus, self.n_bus),nn.Hardsigmoid())

        # self.dva = nn.Sequential(nn.Linear(layer_sizes[-1], self.n_bus), nn.ReLU(), nn.Linear(self.n_bus, self.n_bus),
        #                          nn.Hardsigmoid())
        self.va = nn.Sequential(nn.Linear(self.layers[-1], self.n_bus),  self.activation(), nn.Linear(self.n_bus, self.n_bus), nn.Hardsigmoid())
        
    def _build_net(self):
        layers = []
        layers.extend([nn.Linear(2 * self.n_load, self.layers[0]), self.activation()])
        for i in range(len(self.layers) - 1):
            layers.extend([nn.Linear(self.layers[i], self.layers[i + 1]), self.activation()])
        #layers.append(nn.Linear(self.layers[i], 2 * self.n_bus + 2 * self.n_gen))    
        return nn.Sequential(*layers)
    
    def forward(self, x):
        
        out = self.net(x)
        pg = self.pg(out)
        qg = self.qg(out)
        vm = self.vm(out)
        va = self.va(out)

        return pg, qg, vm, va
    
    def forward(self, x):
        
        out = self.net(x)
        pg = self.pg(out)
        qg = self.qg(out)
        vm = self.vm(out)
        va = self.va(out)

        return pg, qg, vm, va
        
    
    def compute_loss(self, x, y, DATA_bounds, DATA_balance, Ybus, c, 
                     NODE_models, t, epoch, batch_idx, args, collected, saved, device ):
        #print("Batch idx: ", batch_idx)
        # y_pred = self.net(x[:, :2*self.n_load].float())
        # Pg_pred = y_pred[:, :self.n_gen]
        # Qg_pred = y_pred[:, self.n_gen:2 * self.n_gen]
        # V_pred = y_pred[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus]
        # theta_pred = y_pred[:, 2 * self.n_gen + self.n_bus: ]
        Pg_pred, Qg_pred, V_pred, theta_pred = self.forward(x[:, :2*self.n_load].float())

        Pg_pred = get_Pg_bounded(Pg_pred, DATA_bounds, device).to(device).float()
        Qg_pred = get_Qg_bounded(Qg_pred, DATA_bounds, device).to(device).float()
        V_pred = get_Vg_bounded(V_pred, DATA_bounds, device).to(device).float()

        loss_instability = torch.tensor([0]).to(device)
            
        tmp = (balance_constr(y[:, :self.n_gen], y[:, self.n_gen:2 * self.n_gen], y[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus], y[: , 2 * self.n_gen + self.n_bus: ], x[:, :self.n_load].float(), x[:, self.n_load:2*self.n_load].float() , DATA_balance, Ybus, device) - balance_constr(Pg_pred, Qg_pred, V_pred, theta_pred, x[:, :self.n_load].float(), x[:, self.n_load:2*self.n_load].float() , DATA_balance, Ybus, device))**2
        
        loss_balance = tmp if tmp<=1 else torch.pow(tmp,0.5)

        loss_bc = bound_constraint(Pg_pred, Qg_pred, V_pred, theta_pred, DATA_bounds, device = device)
        loss_pred_p = F.mse_loss(Pg_pred, y[:, :self.n_gen])
        loss_pred_q = F.mse_loss(Qg_pred, y[:, self.n_gen:2 * self.n_gen])
        loss_pred_V = F.mse_loss(V_pred, y[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus])
        loss_pred_theta = F.mse_loss(theta_pred, y[: , 2 * self.n_gen + self.n_bus: ])
        loss = loss_pred_p + loss_pred_q + loss_pred_V + loss_pred_theta + c[0] * loss_balance + c[1] * loss_bc + c[2] * loss_instability
        return loss, [loss_pred_p.item(), loss_pred_q.item(), loss_pred_V.item(), loss_pred_theta.item(), loss_balance.item(), loss_bc.item(), loss_instability.item()] #], collected #, total_true_unstable, total_false_unstable, total_detected_true_unstable



    def compute_loss_and_stats(self, x, y, DATA_bounds, DATA_balance, Ybus, c, 
                     NODE_models, t, epoch, batch_idx, args, collected, saved, device ):
        #print("Batch idx: ", batch_idx)
        # y_pred = self.net(x[:, :2*self.n_load].float())
        # Pg_pred = y_pred[:, :self.n_gen]
        # Qg_pred = y_pred[:, self.n_gen:2 * self.n_gen]
        # V_pred = y_pred[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus]
        # theta_pred = y_pred[:, 2 * self.n_gen + self.n_bus: ]
        Pg_pred, Qg_pred, V_pred, theta_pred = self.forward(x[:, :2*self.n_load].float())

        Pg_pred = get_Pg_bounded(Pg_pred, DATA_bounds, device).to(device).float()
        Qg_pred = get_Qg_bounded(Qg_pred, DATA_bounds, device).to(device).float()
        V_pred = get_Vg_bounded(V_pred, DATA_bounds, device).to(device).float()

        loss_instability = torch.tensor([0]).to(device)
            
        tmp = (balance_constr(y[:, :self.n_gen], y[:, self.n_gen:2 * self.n_gen], y[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus], y[: , 2 * self.n_gen + self.n_bus: ], x[:, :self.n_load].float(), x[:, self.n_load:2*self.n_load].float() , DATA_balance, Ybus, device) - balance_constr(Pg_pred, Qg_pred, V_pred, theta_pred, x[:, :self.n_load].float(), x[:, self.n_load:2*self.n_load].float() , DATA_balance, Ybus, device))**2
        
        loss_balance = tmp if tmp<=1 else torch.pow(tmp,0.5)

        loss_bc = bound_constraint(Pg_pred, Qg_pred, V_pred, theta_pred, DATA_bounds, device = device)
        loss_pred_p = F.mse_loss(Pg_pred, y[:, :self.n_gen])
        loss_pred_q = F.mse_loss(Qg_pred, y[:, self.n_gen:2 * self.n_gen])
        loss_pred_V = F.mse_loss(V_pred, y[:, 2 * self.n_gen:2 * self.n_gen + self.n_bus])
        loss_pred_theta = F.mse_loss(theta_pred, y[: , 2 * self.n_gen + self.n_bus: ])
        loss = loss_pred_p + loss_pred_q + loss_pred_V + loss_pred_theta + c[0] * loss_balance + c[1] * loss_bc + c[2] * loss_instability
        return loss, [loss_pred_p.item(), loss_pred_q.item(), loss_pred_V.item(), loss_pred_theta.item(), loss_balance.item(), loss_bc.item(), loss_instability.item()], torch.cat((Pg_pred, Qg_pred, V_pred, theta_pred),1) #y_pred_pred #], collected #, total_true_unstable, total_false_unstable, total_detected_true_unstable



    def train(self, x_train, y_train, x_valid, y_valid, x_test, y_test, DATA_bounds, DATA_balance, Ybus,
              NODE_models, t, args,
              n_epoch, lr, scheduler, optimizer, batch_size, device = 'cpu'):
        
        ### LAGRANGIAN MULTIPLIERS

        c = torch.tensor([args['lambda_zero_balance'], args['lambda_zero_boundaries'], args['lambda_zero_instability']], dtype=torch.float32, device=device)
        
        ### OPTIMIZER AND SCHEDULER

        optimizer = optimizer(self.parameters(), lr=lr)
        scheduler = scheduler(optimizer, milestones=[int(0.5 * n_epoch), int(0.75 * n_epoch)], gamma=0.1)

        ### DATALOADERS

        train_dataset = TensorDataset(x_train, y_train)
        val_dataset = TensorDataset(x_valid, y_valid)
        test_dataset = TensorDataset(x_test, y_test)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)

        ### EARLY STOPPING PARAMETERS & VARIABLES

        max_patience = 100
        patience = 0
        best = 100000

        ### LISTS FOR SAVING LOSS FUNCTION COMPONENTS LISTS

        train_loss, val_loss , test_loss = [], [], [] ### TOTAL LOSS
        mse_loss_list_train, mse_loss_list_valid, mse_loss_list_test = [], [], [] ### MSE LOSS
        balance_loss_list_train, balance_loss_list_valid, balance_loss_list_test = [], [], [] ### BALANCE CONSTRAINT LOSS
        bc_loss_list_train, bc_loss_list_valid, bc_loss_list_test = [], [], [] ### BOUNDARIES CONSTRAINT LOSS
        unstable_loss_list_train, unstable_loss_list_valid, unstable_loss_list_test = [], [], [] ### INSTABILITY PENALTY LOSS
        c_0_list, c_1_list, c_2_list  = [], [], []
        true_unstable_list_train, false_unstable_list_train, detected_unstable_list_train = [], [], [] ## TRUE, FALSE AND DETECTED UNSTABLE TRAJ
        true_unstable_list_valid, false_unstable_list_valid, detected_unstable_list_valid = [], [], []
        true_unstable_list_test, false_unstable_list_test, detected_unstable_list_test = [], [], []

        collected = 0
        saved = False

        for epoch in range(n_epoch):
            batch_loss_train = []
            batch_loss_val = []
            batch_mse_loss_list, batch_balance_loss_list, batch_bc_loss_list, batch_unstable_loss_list = [], [], [], []
            print("Epoch: ", epoch)

            ### TRAINING ROUTINE

            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                loss, loss_item = self.compute_loss(data, target, DATA_bounds, DATA_balance, Ybus, c, NODE_models, t, epoch, batch_idx, args, collected, saved, device)
                loss.backward()
                batch_mse_loss_list.append(loss_item[0]+loss_item[1]+loss_item[2]+loss_item[3])
                batch_balance_loss_list.append(loss_item[4])
                batch_bc_loss_list.append(loss_item[5])
                batch_unstable_loss_list.append(loss_item[6])
                #true_unstable_list_train.append(tu.item())
                #false_unstable_list_train.append(fu.item())
                #detected_true_unstable_list_train.append(dtu.item())
                optimizer.step()
                batch_loss_train.append(loss.item())
            
            ### SAVING AVERAGE TRAINING LOSS (PER EPOCH) COMPONENT 
                
            mse_loss_list_train.append(np.mean(batch_mse_loss_list))
            balance_loss_list_train.append(np.mean(batch_balance_loss_list))
            bc_loss_list_train.append(np.mean(batch_bc_loss_list)) 
            unstable_loss_list_train.append(np.mean(batch_unstable_loss_list))
            train_loss.append(np.mean(batch_loss_train))

            if epoch % 1 == 0:
                with torch.no_grad():

                    ### VALIDATION ROUTINE

                    for x_val, y_val in val_loader:
                        loss_val, loss_item  = self.compute_loss(x_val, y_val, DATA_bounds, DATA_balance, Ybus, c, NODE_models, t, epoch, -1, args, collected, saved, device)
                        mse_loss_list_valid.append(loss_item[0]+loss_item[1]+loss_item[2]+loss_item[3])
                        balance_loss_list_valid.append(loss_item[4])
                        bc_loss_list_valid.append(loss_item[5])
                        unstable_loss_list_valid.append(loss_item[6])
                        batch_loss_val.append(loss_val.item())
                val_loss.append(np.mean(batch_loss_val))
                
                ### LAGRANGIAN MULTIPLIERS UPDATE

                c[0] = c[0] + args['ldstepsize'] * loss_item[-3]  ### BALANCE COSTRAINT MULTIPLIER
                c[1] = c[1] + args['ldstepsize'] * loss_item[-2]  ### BOUNDARY CONSTRAINT MULTIPLIER
                c[2] = c[2] + args['ldstepsize'] * loss_item[-1]  ### INSTABILITY PENALTY MULTIPLIER
                
                c_0_list.append(c[0].item())
                c_1_list.append(c[1].item())
                c_2_list.append(c[2].item())
                
            if loss_val<best:
                best=loss_val
                ### SAVING BEST MODELS

                torch.save(self.state_dict(), f'best_model_SS/model_OPF_{args["id"]}.pth')
                patience = 0
            else:
                patience += 1
                scheduler.step()
                if patience == max_patience:
                    break
        
        ### TEST ROUTINE
        
        self.load_state_dict(torch.load(f'best_model_SS/model_OPF_{args["id"]}.pth'))
        batch_loss_test = []
        with torch.no_grad():
            for x_test, y_test in test_loader:
                loss_test, loss_item  = self.compute_loss(x_test, y_test, DATA_bounds, DATA_balance, Ybus, c, NODE_models, t, epoch, -10, args, collected, saved, device)
                mse_loss_list_test.append(loss_item[0]+loss_item[1]+loss_item[2]+loss_item[3])
                balance_loss_list_test.append(loss_item[4])
                bc_loss_list_test.append(loss_item[5])
                unstable_loss_list_test.append(loss_item[6])
                batch_loss_test.append(loss_test.item())
            test_loss.append(np.mean(batch_loss_test))

        ### COMPUTE STATISTICS PER LOAD
        
        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
        batch_loss_test = []
        
        n_bins = 100
        pg_mse, v_mse, theta_mse = np.zeros((len(test_loader),n_bins)), np.zeros((len(test_loader),n_bins)), np.zeros((len(test_loader),n_bins))
        balance_error, bc_error, unstable_error = np.zeros((len(test_loader),n_bins)), np.zeros((len(test_loader),n_bins)), np.zeros((len(test_loader),n_bins))
        optimality_gap = np.zeros((len(test_loader),n_bins))

        sd_interval_length = 1.9  ### MAX_SD_MAX - MIN_SD_MIN 
        sd_max = 2.5 ### ACROSS ALL LOADS
        gen_cost = torch.tensor([285/250, 285/270, 1]).to(device)

        with torch.no_grad():
            for n_sample,(x_test, y_test) in enumerate(test_loader):
                loss_test, loss_item, y_pred  = self.compute_loss_and_stats(x_test, y_test, DATA_bounds, DATA_balance, Ybus, c, NODE_models, t, epoch, n_sample, args, collected, saved, device)
                x_test = x_test.squeeze(0)
                avg_load = sum([(x_test[i]**2+x_test[n_loads+i]**2)/3 for i in range(n_loads)]).item()
                tmp = int(n_bins/sd_interval_length*(sd_max-avg_load))
                #print("Sd**2: ", x_test[idx]**2+x_test[idx+n_loads]**2)
                if tmp<0 or tmp>n_bins:
                    print("OOOPS")
                pg_mse[n_sample, tmp] = loss_item[0]
                v_mse[n_sample, tmp] = loss_item[2]
                theta_mse[n_sample, tmp] = loss_item[3]
                balance_error[n_sample, tmp] = loss_item[4]
                bc_error[n_sample, tmp] = loss_item[5]
                unstable_error[n_sample, tmp] = loss_item[6]
                relative_opt_gap = torch.abs(torch.sum(gen_cost*(y_pred.squeeze(0)[:n_gen]-y_test.squeeze(0)[:n_gen])))/torch.sum(gen_cost*(y_test.squeeze(0)[:n_gen]))
                optimality_gap[n_sample, tmp] = relative_opt_gap.item()*100
        
        avg_pg_mse, avg_v_mse, avg_theta_mse, avg_balance_error = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)
        avg_bc_error, avg_unstable_error, avg_optimality_gap  = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)
        
        std_dev_pg_mse, std_dev_v_mse, std_dev_theta_mse, std_dev_balance_error = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)
        std_dev_bc_error, std_dev_unstable_error, std_dev_optimality_gap  = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)
        
        ### shape: (n_bins)

        avg_pg_mse = pg_mse.mean(axis=0)
        std_dev_pg_mse = pg_mse.std(axis=0)

        avg_v_mse = v_mse.mean(axis=0)
        std_dev_v_mse = v_mse.std(axis=0)

        avg_theta_mse = theta_mse.mean(axis=0)
        std_dev_theta_mse = theta_mse.std(axis=0)

        avg_balance_error = balance_error.mean(axis=0)
        std_dev_balance_error = balance_error.std(axis=0)

        avg_bc_error = bc_error.mean(axis=0)
        std_dev_bc_error = bc_error.std(axis=0)

        avg_unstable_error = unstable_error.mean(axis=0)
        std_dev_unstable_error = unstable_error.std(axis=0)

        avg_optimality_gap = optimality_gap.mean(axis=0)
        std_dev_optimality_gap = optimality_gap.std(axis=0)
        
        ###############
        #
        ############### SAVING RESULTS  ###############
        #
        ###############

        direc='LD_SS'
        run_id = args['id'] 

        save_data(direc, run_id, avg_pg_mse, std_dev_pg_mse, avg_v_mse, std_dev_v_mse, avg_theta_mse, std_dev_theta_mse,
                  avg_balance_error, std_dev_balance_error, avg_bc_error, std_dev_bc_error, avg_unstable_error, std_dev_unstable_error, avg_optimality_gap, std_dev_optimality_gap)

        save_lists(direc, run_id, c_0_list, c_1_list, c_2_list, train_loss, mse_loss_list_train, balance_loss_list_train, bc_loss_list_train, unstable_loss_list_train,
                val_loss, mse_loss_list_valid, balance_loss_list_valid, bc_loss_list_valid, unstable_loss_list_valid,
                test_loss, mse_loss_list_test, balance_loss_list_test, bc_loss_list_test, unstable_loss_list_test,
                true_unstable_list_train, false_unstable_list_train, detected_unstable_list_train, 
                true_unstable_list_valid, false_unstable_list_valid, detected_unstable_list_valid,
                true_unstable_list_test, false_unstable_list_test, detected_unstable_list_test)
        
            
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='PI_ROPF')

    ### HYPERPARAMS

    parser.add_argument('--seed', type=int, default=123, help='random seed')
    parser.add_argument('--nHiddenUnit_SS', type=int, default = 50, help='number of hidden units')
    parser.add_argument('--nHiddenUnit_NODE', type=int, default = 200, help='number of hidden units') #50 for the old models
    parser.add_argument('--activation_NODE', type=str, default = "RELU", help='activation_function')
    parser.add_argument('--activation_SS', type=str, default = "RELU", help='activation_function')
    parser.add_argument('--optimizer', type=int, default = 2, help='GD algorithm')
    parser.add_argument('--lr', type=float, default = 1e-3, help='total number of datapoints')
    parser.add_argument('--batchsize', type=int, default = 10, help='training batch size') #10
    parser.add_argument('--epochs', type=int, default = 1500, help='training batch size')
    parser.add_argument('--initSplit', type=float, default = .1)
    parser.add_argument('--normalize', type=bool, default = False)
    parser.add_argument('--plot', type=bool, default = False)
    parser.add_argument('--useFFLayer', type=bool, default = True)
    parser.add_argument('--k', type=float, default = .05)
    parser.add_argument('--k_check', type=float, default = .5)
    parser.add_argument('--delta_k', type=float, default = .05)
    parser.add_argument('--delta_k_epoch', type=int, default = 10)
    parser.add_argument('--delta_k_split', type=float, default = 1)
    parser.add_argument('--ldstepsize', type=float, default = .01)
    parser.add_argument('--activate_instability_computation_epoch', type=int, default = 4)
    parser.add_argument('--max_patience', type=int, default = 10)
    parser.add_argument('--nLayer_NODE', type=int, default = 5, help='number of layers')
    parser.add_argument('--nLayer_SS', type=int, default = 5, help='number of layers')
    parser.add_argument('--id', type=int, default = 999, help='number of layers')
    parser.add_argument('--lambda_zero_balance', type=float, default = .1, help='lambda(0)[0]')
    parser.add_argument('--lambda_zero_boundaries', type=float, default = .1, help='lambda(0)[1]')
    parser.add_argument('--lambda_zero_instability', type=float, default = 1, help='lambda(0)[2]')

    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    today = date.today()
    
    args = parser.parse_args()
    args = vars(args) # change to dictionary

    # NETWORK PARAMETERS

    n_bus = 9 #14
    n_gen = 3 #5
    n_loads = 3 #11
    arch_list = [args['nHiddenUnit_SS']] * args['nLayer_SS']
    hidden_layers_nodes = arch_list

    arch_list_NODE = [args['nHiddenUnit_NODE']] * args['nLayer_NODE']
    arch_list_NODE.insert(0,4) 
    arch_list_NODE.append(4)

    T = 10  
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # DATA LOADING

    data, DATA_bounds, DATA_balance, Ybus = read_data(n_bus, n_loads, n_gen)
    DATA_bounds["V_max"][0:n_gen] = 1.1

    Ybus = Ybus.to(device)

    # TRAIN, VALIDATION, TEST SPLIT
    train_data = data.iloc[:int(0.8 * len(data))]
    valid_data = data.iloc[int(0.8 * len(data)):int(0.9 * len(data)), :]
    test_data = data.iloc[int(0.9 * len(data)):, :]

    ### DOWNSAMPLING
    train_data = train_data[:int(.1*train_data.shape[0])]
    valid_data = valid_data[:int(.1*train_data.shape[0])]
    test_data = test_data[:int(.1*train_data.shape[0])]

    # TRAINING SET
    train_x = torch.tensor(train_data.iloc[:, 0:2*n_loads].values, dtype=torch.float32)/100
    train_y = torch.tensor(train_data.iloc[:, 2*n_loads: ].values, dtype=torch.float32)
    train_x = train_x.to(device)
    train_y = train_y.to(device)

    # VALIDATION SET
    valid_x = torch.tensor(valid_data.iloc[:, 0:2*n_loads].values, dtype=torch.float32)/100
    valid_y = torch.tensor(valid_data.iloc[:, 2*n_loads: ].values, dtype=torch.float32)
    valid_x = valid_x.to(device)
    valid_y = valid_y.to(device)

    # TEST SET
    test_x = torch.tensor(test_data.iloc[:, 0:2*n_loads].values, dtype=torch.float32)/100
    test_y = torch.tensor(test_data.iloc[:, 2*n_loads: ].values, dtype=torch.float32)
    test_x = test_x.to(device)
    test_y = test_y.to(device)

    all_loads = torch.cat((train_x, valid_x, test_x)) 


    # LOAD PROFILE (USED FOR STATISTICS)
    '''
    bins = 20
    cmap_name = 'load_gradient'
    colors = [(0, 0.8, 1), (0, 0, 1)]  # Light blue to dark blue
    n_bins = 30  # Use more bins for a smoother gradient
    load_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)

    ### LOAD RANGE: 1.12<Sd(1)<1.58, 0.78< Sd(2)<1.12, 0.87<Sd(3)<1.25

    for i in range(n_loads):
        print(i)
        # Calculate the histogram data
        hist_data = (all_loads[:, i]**2 + all_loads[:, i+n_loads]**2).detach().numpy()
        counts, bin_edges = np.histogram(hist_data, bins=bins, density=True)
        bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])
        # Normalize the bin centers for the colormap
        norm = plt.Normalize(bin_edges.min(), bin_edges.max())
        # Create the histogram using bar chart
        fig, ax_theta = plt.subplots(figsize=(8, 4))
        for edge_left, edge_right, center in zip(bin_edges[:-1], bin_edges[1:], bin_centers):
            color = load_cmap(norm(center))
            ax_theta.bar(center, counts[np.digitize(center, bin_edges) - 1], align='center',
                        width=np.diff(bin_edges)[0], color=color) #, edgecolor='black')
        # Set titles and labels for theta plot
        ax_theta.set_xlabel('Value')
        ax_theta.set_ylabel('Density')
        ax_theta.legend([f'{i+1} Load'])
        plt.tight_layout()
        plt.show()
    '''

    ### RANDOM INTIAL CONDITION (\delta(0), \omega(0)) GENERATION (per sample)
    init_cond = torch.zeros((train_x.shape[0]+valid_x.shape[0]+test_x.shape[0],n_gen,2)).to(device)
    for i in range( train_x.shape[0] + valid_x.shape[0] + test_x.shape[0]):
        #init_cond[i,0,:] = torch.cat((0.1 * torch.rand(1), 0.005* torch.rand(1)- 0.0025))
        init_cond[i,:,:] = torch.tensor([(0.1 * torch.rand(1), 0.005* torch.rand(1)- 0.0025), (0.1 * torch.rand(1), 0.005* torch.rand(1)- 0.0025), (0.1 * torch.rand(1), 0.005* torch.rand(1)- 0.0025)])

    model = OptimalPowerFlow(hidden_layers_nodes, n_bus, n_gen, n_loads)
    model = model.to(device)
    
    t = torch.linspace(0, T, 1000).reshape(-1, 1).to(device)
    print(os.getcwd())
    NODE_models = {}

    # PRETRAINED NODE MODELS LOADING

    for i in DATA_balance['idx_Gbus'][:3]:
        if i==0:
            arch_list_NODE = [args['nHiddenUnit_NODE']] * 2
        else:
            arch_list_NODE = [args['nHiddenUnit_NODE']] * args['nLayer_NODE']
        arch_list_NODE.insert(0,4) 
        arch_list_NODE.append(4)
        NODE_models[f'gen{i}'] = SwingNN(arch_list_NODE, args['activation_NODE'])
        NODE_models[f'gen{i}'].load_state_dict(torch.load(f'model/new_best_model_{i+1}.pt', map_location=torch.device('cpu')))
        NODE_models[f'gen{i}'] = NODE_models[f'gen{i}'].to(device)
    
    ### TRAINING, VALIDATION AND TEST ROUTINE
    
    model.train(train_x, train_y, valid_x, valid_y, test_x, test_y, DATA_bounds, DATA_balance, Ybus, 
                NODE_models, t, args,
                n_epoch = args['epochs'], lr = args['lr'], scheduler = MultiStepLR, optimizer = optim.AdamW, batch_size = args['batchsize'], device = device)
    
    if args['plot']:
        plot_results(model, test_x[:, :2*n_loads].float(), test_y, DATA_bounds, DATA_balance, Ybus, n_bus, n_gen)
